This project is an AI-powered competitive analysis tool designed for startups. It leverages a LangGraph agent to guide users in refining the scope of their competitive analysis. The tool employs a robust Retrieval-Augmented Generation (RAG) pipeline, incorporating multiple advanced retrieval models such as ColBERT, SPLADE, DPR, BM25-BERT Hybrid, ANCE, MiniLM, and Reformer. This enables the extraction of relevant insights from various online sources like news articles, market reports, blogs, research papers, and more. 

A user-friendly Streamlit-based dashboard provides a seamless interactive experience, allowing users to input their industry, competitors, and key focus areas. The tool then retrieves and presents competitive data through insightful visualizations. Users have the flexibility to iteratively refine their searches and compare the performance of different retrieval models in real-time.

To get started, follow these installation and execution instructions.

Installation:
1. Ensure you have Python (3.x) installed on your Windows 10 machine.
2. Create a virtual environment to manage project dependencies:
   - python -m venv .venv
3. Activate the virtual environment:
   - .venv\Scripts\activate
4. Install the required packages listed in the 'requirements.txt' file:
   - pip install -r requirements.txt
5. Obtain and set your API keys:
   - Get an OpenAI API key and set it as an environment variable named 'OPENAI_API_KEY'. You can do this by adding the following line to your terminal's configuration file (e.g., .bashrc or .zshrc): 
     - export OPENAI_API_KEY=<YOUR_API_KEY>
   - Get a Hugging Face API token and set it as an environment variable named 'HUGGINGFACEHUB_API_TOKEN': 
     - export HUGGINGFACEHUB_API_TOKEN=<YOUR_API_TOKEN>
   - Replace <YOUR_API_KEY> and <YOUR_API_TOKEN> with your actual keys.

Running the Application:

1. Navigate to the 'streamlit_app' directory:
   - cd streamlit_app
2. Execute the Streamlit application:
   - streamlit run dashboard.py
3. Access the dashboard in your web browser at the address shown in the terminal (typically http://localhost:8501).

Directory Structure:

The project is structured as follows:

ğŸ“¦ AI_Competitive_Analysis_Tool/
â”œâ”€â”€ ğŸ“ app/
â”‚   â”œâ”€â”€ ğŸ“ models/
â”‚   â”‚   â”œâ”€â”€ colbert_model.py                # ColBERT retrieval model
â”‚   â”‚   â”œâ”€â”€ splade_model.py                 # SPLADE retrieval model
â”‚   â”‚   â”œâ”€â”€ dpr_model.py                    # DPR retrieval model
â”‚   â”‚   â”œâ”€â”€ bm25_bert_model.py             # BM25-BERT Hybrid model
â”‚   â”‚   â”œâ”€â”€ ance_model.py                   # ANCE model
â”‚   â”‚   â”œâ”€â”€ minilm_model.py                 # MiniLM model
â”‚   â”‚   â””â”€â”€ reformer_model.py              # Reformer model
â”‚   â”œâ”€â”€ ğŸ“ rag_pipeline/
â”‚   â”‚   â”œâ”€â”€ iterative_rag.py                # Iterative RAG pipeline
â”‚   â”‚   â””â”€â”€ retriever.py                    # Base retrieval logic
â”‚   â”œâ”€â”€ ğŸ“ streamlit_app/
â”‚   â”‚   â”œâ”€â”€ dashboard.py                    # Streamlit UI/UX
â”‚   â”‚   â””â”€â”€ visualizations.py               # Data visualizations
â”‚   â”œâ”€â”€ ğŸ“ langgraph_agent/
â”‚   â”‚   â””â”€â”€ langgraph_chatbot.py            # LangGraph agent
â”‚   â”œâ”€â”€ ğŸ“ data_acquisition/
â”‚   â”‚   â”œâ”€â”€ company_website_scraper.py      # Website data scraper
â”‚   â”‚   â”œâ”€â”€ social_media_analyzer.py        # Social media analysis
â”‚   â”‚   â””â”€â”€ financial_data_extractor.py     # Financial data extraction
â”‚   â”œâ”€â”€ ğŸ“ data/
â”‚   â”‚   â”œâ”€â”€ sample_competitive_data.csv     # Sample data
â”‚   â”‚   â””â”€â”€ scraped_results.json            # Scraped results
â”‚   â””â”€â”€ __init__.py                         # App module initialization
â”œâ”€â”€ ğŸ“ config/
â”‚   â”œâ”€â”€ model_config.json                   # Model configurations
â”‚   â””â”€â”€ app_config.json                     # Application settings
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ test_colbert.py                     # ColBERT model tests
â”‚   â”œâ”€â”€ test_splade.py                      # SPLADE model tests
â”‚   â”œâ”€â”€ test_dpr.py                         # DPR model tests
â”‚   â”œâ”€â”€ test_bm25_bert.py                  # BM25-BERT Hybrid model tests
â”‚   â”œâ”€â”€ test_ance.py                        # ANCE model tests
â”‚   â”œâ”€â”€ test_minilm.py                      # MiniLM model tests
â”‚   â”œâ”€â”€ test_reformer.py                   # Reformer model tests
â”‚   â””â”€â”€ test_rag_pipeline.py                # RAG pipeline tests
â”œâ”€â”€ ğŸ“ logs/
â”‚   â”œâ”€â”€ retrieval_log.txt                   # Retrieval operation logs
â”‚   â””â”€â”€ error_log.txt                       # Error logs
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â””â”€â”€ exploratory_analysis.py             # Data exploration
â”œâ”€â”€ ğŸ“ docs/
â”‚   â””â”€â”€ user_guide.md                       # Tool usage documentation
â”œâ”€â”€ requirements.txt                        # Python dependencies
â”œâ”€â”€ setup.py                                # Project setup
â””â”€â”€ README.md                               # Project overview
